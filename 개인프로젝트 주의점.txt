pip install git+https://github.com/openai/whisper.git
mp4 ->mp3
ffmpeg 오디오 파일 처리 
mp3파일을 wav로 변환 
모델 크기

실시간 보이스피싱 음성을 문맥, 또는
문장 단위로 파악하기 위해 5초 단위로 저장한다. 

koelectra
Efficiently Learning an Encoder that Classifies Token Replacements Accurately




KoElectra:
KoElectra는 ELECTRA 아키텍처를 기반으로 하며, "discriminative" 방식으로 학습합니다. 생성기(generator)와 판별기(discriminator)로 구성되어 있으며, 판별기가 입력 문장에서 올바른 단어와 생성기가 예측한 단어를 구분하는 방식으로 학습합니다.

KoVERT:
KoVERT는 BERT(Bidirectional Encoder Representations from Transformers) 아키텍처를 기반으로 하며, 전통적인 마스킹된 언어 모델(Masked Language Model, MLM) 방식으로 학습합니다. 입력 문장에서 무작위로 선택된 단어를 마스킹하고, 이를 예측하는 방식으로 학습합니다.
2. 학습 방식
KoElectra:

KoElectra는 전체 문장에서 모든 단어를 활용하여 판별하는 방식으로 학습하므로, 상대적으로 적은 데이터로도 높은 성능을 달성할 수 있습니다. 이는 판별기가 생성기의 예측을 평가하는 데 중점을 두기 때문입니다.
KoVERT:

KoVERT는 MLM 방식으로, 특정 단어를 마스킹하고 그 마스킹된 단어를 예측하는 데 집중합니다. 이 방식은 각 단어가 독립적으로 예측되므로, 상대적으로 많은 양의 데이터가 필요할 수 있습니다.
3. 성능 및 효율성
KoElectra:

KoElectra는 효율적인 학습 덕분에 빠른 수렴 속도를 보여주며, 적은 데이터로도 경쟁력 있는 성능을 발휘합니다. 다양한 NLP 태스크에서 우수한 결과를 나타내는 경우가 많습니다.
KoVERT:

KoVERT는 BERT의 강력한 성능을 기반으로 하며, 풍부한 표현력을 가지고 있지만, 상대적으로 더 많은 데이터와 시간이 필요할 수 있습니다. BERT 기반 모델들은 대체로 성능이 뛰어나지만, 학습하는 데 필요한 자원이 더 많이 소모될 수 있습니다.